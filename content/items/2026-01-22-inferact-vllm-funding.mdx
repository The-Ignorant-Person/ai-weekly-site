---
title: vLLM 创始团队成立 Inferact，融资 1.5 亿美元用于商业化推理优化 (Inferact)
slug: 2026-01-22-inferact-vllm-funding
createdAt: 2026-01-22
updatedAt: 2026-01-22
sourceDates:
  - 2026-01-22
tags:
  - 工具链
  - 推理优化
  - 融资
evidence: B
score: 8.3
---

## 一句话结论

开源推理库 **vLLM** 的团队成立新公司 **Inferact**，获 1.5 亿美元融资，用于将其低延迟、高吞吐的推理技术商业化，并强调推理效率是大模型规模化的关键【610046326829213†L123-L156】。

## 发生了什么

1. **公司成立与融资**：2026 年 1 月 22 日，TechCrunch 报道 vLLM 项目组核心成员成立新公司 Inferact，获得 Andreessen Horowitz 等投资者领投的 1.5 亿美元融资，估值约 8 亿美元【610046326829213†L123-L156】。
2. **技术特点**：vLLM 利用 **PagedAttention** 和 **连续批处理（continuous batching）** 技术，在 GPU 上实现 2–24 倍的推理速度提升；该项目已被多家企业（包括亚马逊）用于降低服务成本【610046326829213†L123-L156】。
3. **战略意义**：团队将围绕低延迟推理构建商业化产品和服务，目标客户包括采用大型语言模型的公司。报道指出，随着模型规模增大，推理效率直接影响业务利润率【610046326829213†L123-L156】。

## 为什么重要

* **推理瓶颈**：大模型部署常常被推理成本所困扰，vLLM 的优化表明通过改进内存管理和调度策略可以显著提升性能，这对 Java 后端在调用 LLM 服务时降低响应时间非常有价值。
* **开源到商业化**：这一融资标志着开源推理优化工具的商业前景，为开发者提供了更多可选择的基础设施供应商，也意味着竞争将集中在如何降低运行成本和提供企业级支持。
* **生态合作**：投资方包括多家知名 VC 和云服务提供商，显示资本重视推理层创新，后端工程师应关注 API 与接口标准化，以便未来灵活替换服务商。

## 证据与可信度

TechCrunch 提供的采访和融资信息属可靠媒体报道，但关于技术细节的描述主要引用项目方自述，因此证据级别评为 **B**【610046326829213†L123-L156】。

## 对我有什么用

1. **优化服务端推理**：测试 vLLM 的 PagedAttention 和连续批处理特性，将其集成到 Java 服务，实测性能提升，并编写示例代码展示多请求合批推理。
2. **监测商业化 API**：关注 Inferact 将推出的推理云服务，比较其价格、延迟和易用性，与自部署方案进行权衡，为项目选择合适的推理基础设施。
3. **关注社区与贡献**：作为开源项目用户，可参与 vLLM 社区贡献 bug 修复和特性，实现对工具链的深入理解并丰富简历。

## 风险与不确定性

* **商业模式未明朗**：新公司的定价和服务策略尚未公布，未来可能会收取高昂费用或限制某些使用场景。
* **技术适配**：vLLM 的优化主要针对 GPU 推理，可能不适用于 CPU 或小型部署环境，需要评估硬件适配性。
* **兼容性问题**：使用连续批处理需要调整请求队列和输入格式，对现有系统架构有一定挑战。

## 来源链接

* TechCrunch 文章《Creators of open-source vLLM project form Inferact, raise $150M to commercialize AI inference optimization》（2026‑01‑22，免费）【610046326829213†L123-L156】。
* vLLM 官方仓库和文档（开源，免费）。