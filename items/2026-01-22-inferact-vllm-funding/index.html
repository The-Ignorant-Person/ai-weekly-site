<!DOCTYPE html><html><head><meta charSet="UTF-8" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1.0" data-next-head=""/><title data-next-head="">vLLM 创始团队成立 Inferact，融资 1.5 亿美元用于商业化推理优化 (Inferact)</title><link rel="preload" href="/ai-weekly-site/_next/static/chunks/60f40cacfadee559.css" as="style"/><link rel="stylesheet" href="/ai-weekly-site/_next/static/chunks/60f40cacfadee559.css" data-n-g=""/><noscript data-n-css=""></noscript><script src="/ai-weekly-site/_next/static/chunks/43be37b4a0916711.js" defer=""></script><script src="/ai-weekly-site/_next/static/chunks/546ec87c411f3ff8.js" defer=""></script><script src="/ai-weekly-site/_next/static/chunks/ed036ec145a02503.js" defer=""></script><script src="/ai-weekly-site/_next/static/chunks/turbopack-0e17fd22a0fc7eb0.js" defer=""></script><script src="/ai-weekly-site/_next/static/chunks/50466d65272e8564.js" defer=""></script><script src="/ai-weekly-site/_next/static/chunks/turbopack-eb310b673e14aadc.js" defer=""></script><script src="/ai-weekly-site/_next/static/wt1dVk6nNgzPoDFmJrGAb/_ssgManifest.js" defer=""></script><script src="/ai-weekly-site/_next/static/wt1dVk6nNgzPoDFmJrGAb/_buildManifest.js" defer=""></script></head><body><div id="__next"><header class="header"><nav><a href="/ai-weekly-site/"><strong>AI 周报站点</strong></a><a href="/ai-weekly-site/weeks/">周报归档</a><a href="/ai-weekly-site/tags/">标签</a><a href="/ai-weekly-site/search/">搜索</a></nav></header><main class="container"><article><h1>vLLM 创始团队成立 Inferact，融资 1.5 亿美元用于商业化推理优化 (Inferact)</h1><p>创建时间：<!-- -->2026-01-22T00:00:00.000Z<!-- -->；更新时间：<!-- -->2026-01-22T00:00:00.000Z</p><div><a class="tag-chip" href="/ai-weekly-site/tags/%E5%B7%A5%E5%85%B7%E9%93%BE/">工具链</a><a class="tag-chip" href="/ai-weekly-site/tags/%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96/">推理优化</a><a class="tag-chip" href="/ai-weekly-site/tags/%E8%9E%8D%E8%B5%84/">融资</a><span class="badge B">B</span></div><h2>一句话结论</h2>
<p>开源推理库 <strong>vLLM</strong> 的团队成立新公司 <strong>Inferact</strong>，获 1.5 亿美元融资，用于将其低延迟、高吞吐的推理技术商业化，并强调推理效率是大模型规模化的关键【610046326829213†L123-L156】。</p>
<h2>发生了什么</h2>
<ol>
<li><strong>公司成立与融资</strong>：2026 年 1 月 22 日，TechCrunch 报道 vLLM 项目组核心成员成立新公司 Inferact，获得 Andreessen Horowitz 等投资者领投的 1.5 亿美元融资，估值约 8 亿美元【610046326829213†L123-L156】。</li>
<li><strong>技术特点</strong>：vLLM 利用 <strong>PagedAttention</strong> 和 <strong>连续批处理（continuous batching）</strong> 技术，在 GPU 上实现 2–24 倍的推理速度提升；该项目已被多家企业（包括亚马逊）用于降低服务成本【610046326829213†L123-L156】。</li>
<li><strong>战略意义</strong>：团队将围绕低延迟推理构建商业化产品和服务，目标客户包括采用大型语言模型的公司。报道指出，随着模型规模增大，推理效率直接影响业务利润率【610046326829213†L123-L156】。</li>
</ol>
<h2>为什么重要</h2>
<ul>
<li><strong>推理瓶颈</strong>：大模型部署常常被推理成本所困扰，vLLM 的优化表明通过改进内存管理和调度策略可以显著提升性能，这对 Java 后端在调用 LLM 服务时降低响应时间非常有价值。</li>
<li><strong>开源到商业化</strong>：这一融资标志着开源推理优化工具的商业前景，为开发者提供了更多可选择的基础设施供应商，也意味着竞争将集中在如何降低运行成本和提供企业级支持。</li>
<li><strong>生态合作</strong>：投资方包括多家知名 VC 和云服务提供商，显示资本重视推理层创新，后端工程师应关注 API 与接口标准化，以便未来灵活替换服务商。</li>
</ul>
<h2>证据与可信度</h2>
<p>TechCrunch 提供的采访和融资信息属可靠媒体报道，但关于技术细节的描述主要引用项目方自述，因此证据级别评为 <strong>B</strong>【610046326829213†L123-L156】。</p>
<h2>对我有什么用</h2>
<ol>
<li><strong>优化服务端推理</strong>：测试 vLLM 的 PagedAttention 和连续批处理特性，将其集成到 Java 服务，实测性能提升，并编写示例代码展示多请求合批推理。</li>
<li><strong>监测商业化 API</strong>：关注 Inferact 将推出的推理云服务，比较其价格、延迟和易用性，与自部署方案进行权衡，为项目选择合适的推理基础设施。</li>
<li><strong>关注社区与贡献</strong>：作为开源项目用户，可参与 vLLM 社区贡献 bug 修复和特性，实现对工具链的深入理解并丰富简历。</li>
</ol>
<h2>风险与不确定性</h2>
<ul>
<li><strong>商业模式未明朗</strong>：新公司的定价和服务策略尚未公布，未来可能会收取高昂费用或限制某些使用场景。</li>
<li><strong>技术适配</strong>：vLLM 的优化主要针对 GPU 推理，可能不适用于 CPU 或小型部署环境，需要评估硬件适配性。</li>
<li><strong>兼容性问题</strong>：使用连续批处理需要调整请求队列和输入格式，对现有系统架构有一定挑战。</li>
</ul>
<h2>来源链接</h2>
<ul>
<li>TechCrunch 文章《Creators of open-source vLLM project form Inferact, raise $150M to commercialize AI inference optimization》（2026‑01‑22，免费）【610046326829213†L123-L156】。</li>
<li>vLLM 官方仓库和文档（开源，免费）。</li>
</ul></article></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontMatter":{"title":"vLLM 创始团队成立 Inferact，融资 1.5 亿美元用于商业化推理优化 (Inferact)","slug":"2026-01-22-inferact-vllm-funding","createdAt":"2026-01-22T00:00:00.000Z","updatedAt":"2026-01-22T00:00:00.000Z","sourceDates":["2026-01-22T00:00:00.000Z"],"tags":["工具链","推理优化","融资"],"evidence":"B","score":8.3},"mdxSource":{"compiledSource":"\"use strict\";\nconst {Fragment: _Fragment, jsx: _jsx, jsxs: _jsxs} = arguments[0];\nconst {useMDXComponents: _provideComponents} = arguments[0];\nfunction _createMdxContent(props) {\n  const _components = {\n    h2: \"h2\",\n    li: \"li\",\n    ol: \"ol\",\n    p: \"p\",\n    strong: \"strong\",\n    ul: \"ul\",\n    ..._provideComponents(),\n    ...props.components\n  };\n  return _jsxs(_Fragment, {\n    children: [_jsx(_components.h2, {\n      children: \"一句话结论\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"开源推理库 \", _jsx(_components.strong, {\n        children: \"vLLM\"\n      }), \" 的团队成立新公司 \", _jsx(_components.strong, {\n        children: \"Inferact\"\n      }), \"，获 1.5 亿美元融资，用于将其低延迟、高吞吐的推理技术商业化，并强调推理效率是大模型规模化的关键【610046326829213†L123-L156】。\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"发生了什么\"\n    }), \"\\n\", _jsxs(_components.ol, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"公司成立与融资\"\n        }), \"：2026 年 1 月 22 日，TechCrunch 报道 vLLM 项目组核心成员成立新公司 Inferact，获得 Andreessen Horowitz 等投资者领投的 1.5 亿美元融资，估值约 8 亿美元【610046326829213†L123-L156】。\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"技术特点\"\n        }), \"：vLLM 利用 \", _jsx(_components.strong, {\n          children: \"PagedAttention\"\n        }), \" 和 \", _jsx(_components.strong, {\n          children: \"连续批处理（continuous batching）\"\n        }), \" 技术，在 GPU 上实现 2–24 倍的推理速度提升；该项目已被多家企业（包括亚马逊）用于降低服务成本【610046326829213†L123-L156】。\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"战略意义\"\n        }), \"：团队将围绕低延迟推理构建商业化产品和服务，目标客户包括采用大型语言模型的公司。报道指出，随着模型规模增大，推理效率直接影响业务利润率【610046326829213†L123-L156】。\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"为什么重要\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"推理瓶颈\"\n        }), \"：大模型部署常常被推理成本所困扰，vLLM 的优化表明通过改进内存管理和调度策略可以显著提升性能，这对 Java 后端在调用 LLM 服务时降低响应时间非常有价值。\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"开源到商业化\"\n        }), \"：这一融资标志着开源推理优化工具的商业前景，为开发者提供了更多可选择的基础设施供应商，也意味着竞争将集中在如何降低运行成本和提供企业级支持。\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"生态合作\"\n        }), \"：投资方包括多家知名 VC 和云服务提供商，显示资本重视推理层创新，后端工程师应关注 API 与接口标准化，以便未来灵活替换服务商。\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"证据与可信度\"\n    }), \"\\n\", _jsxs(_components.p, {\n      children: [\"TechCrunch 提供的采访和融资信息属可靠媒体报道，但关于技术细节的描述主要引用项目方自述，因此证据级别评为 \", _jsx(_components.strong, {\n        children: \"B\"\n      }), \"【610046326829213†L123-L156】。\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"对我有什么用\"\n    }), \"\\n\", _jsxs(_components.ol, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"优化服务端推理\"\n        }), \"：测试 vLLM 的 PagedAttention 和连续批处理特性，将其集成到 Java 服务，实测性能提升，并编写示例代码展示多请求合批推理。\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"监测商业化 API\"\n        }), \"：关注 Inferact 将推出的推理云服务，比较其价格、延迟和易用性，与自部署方案进行权衡，为项目选择合适的推理基础设施。\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"关注社区与贡献\"\n        }), \"：作为开源项目用户，可参与 vLLM 社区贡献 bug 修复和特性，实现对工具链的深入理解并丰富简历。\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"风险与不确定性\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"商业模式未明朗\"\n        }), \"：新公司的定价和服务策略尚未公布，未来可能会收取高昂费用或限制某些使用场景。\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"技术适配\"\n        }), \"：vLLM 的优化主要针对 GPU 推理，可能不适用于 CPU 或小型部署环境，需要评估硬件适配性。\"]\n      }), \"\\n\", _jsxs(_components.li, {\n        children: [_jsx(_components.strong, {\n          children: \"兼容性问题\"\n        }), \"：使用连续批处理需要调整请求队列和输入格式，对现有系统架构有一定挑战。\"]\n      }), \"\\n\"]\n    }), \"\\n\", _jsx(_components.h2, {\n      children: \"来源链接\"\n    }), \"\\n\", _jsxs(_components.ul, {\n      children: [\"\\n\", _jsx(_components.li, {\n        children: \"TechCrunch 文章《Creators of open-source vLLM project form Inferact, raise $150M to commercialize AI inference optimization》（2026‑01‑22，免费）【610046326829213†L123-L156】。\"\n      }), \"\\n\", _jsx(_components.li, {\n        children: \"vLLM 官方仓库和文档（开源，免费）。\"\n      }), \"\\n\"]\n    })]\n  });\n}\nfunction MDXContent(props = {}) {\n  const {wrapper: MDXLayout} = {\n    ..._provideComponents(),\n    ...props.components\n  };\n  return MDXLayout ? _jsx(MDXLayout, {\n    ...props,\n    children: _jsx(_createMdxContent, {\n      ...props\n    })\n  }) : _createMdxContent(props);\n}\nreturn {\n  default: MDXContent\n};\n","frontmatter":{},"scope":{}}},"__N_SSG":true},"page":"/items/[slug]","query":{"slug":"2026-01-22-inferact-vllm-funding"},"buildId":"wt1dVk6nNgzPoDFmJrGAb","assetPrefix":"/ai-weekly-site","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>